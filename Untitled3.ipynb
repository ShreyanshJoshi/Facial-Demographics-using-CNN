{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled2.ipynb",
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "1vUKViRkhJdR7U4S9C2TyBpPwJotW1oxu",
      "authorship_tag": "ABX9TyMKI3YjycB0n3AWfq7o7LGM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShreyanshJoshi/Temp/blob/master/Untitled3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZXWen-qYTnFz",
        "colab_type": "text"
      },
      "source": [
        "The original work consumed face pictures collected from IMDB (7 GB) and Wikipedia (1 GB).In this implementation, we use Wikipedia (1 GB) dataset.\n",
        "The dataset has been zipped and uploaded into Google Drive, which in turn has been mounted onto Google Colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kfeb5BkqkiYR",
        "colab_type": "code",
        "outputId": "29764476-b13f-44de-c657-e65e8cd954fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "!unzip \"/content/drive/My Drive/wiki_crop.zip\"                    # Unzipping the zip file, so that it can be referred to by code"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  /content/drive/My Drive/wiki_crop.zip\n",
            "replace wiki_crop/02/9994102_1992-03-13_2013.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TwcR7Qa3qYmG",
        "colab_type": "code",
        "outputId": "d47e838a-6349-41da-e25a-b80bbf229eb1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import scipy.io\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from keras import applications,activations\n",
        "from keras.preprocessing.image import ImageDataGenerator,load_img,img_to_array\n",
        "from keras import optimizers,utils\n",
        "from keras.models import Sequential, Model \n",
        "from keras.layers import Dropout, Flatten, Dense, GlobalAveragePooling2D,BatchNormalization,ZeroPadding2D\n",
        "from keras.layers import Convolution2D, Activation,MaxPooling2D\n",
        "from keras import backend as k \n",
        "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, TensorBoard, EarlyStopping"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CGmYB36UqrIc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mat = scipy.io.loadmat('wiki_crop/wiki.mat')\n",
        "# Reading Matlab files with SciPy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JVU2JxALqufP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Converting into pandas dataframe. Always easier and better to deal with :)\n",
        "\n",
        "instances = mat['wiki'][0][0][0].shape[1]\n",
        " \n",
        "columns = [\"dob\", \"photo_taken\", \"full_path\", \"gender\", \"name\", \"face_location\", \"face_score\", \"second_face_score\"]\n",
        " \n",
        "df = pd.DataFrame(index = range(0,instances), columns = columns)\n",
        " \n",
        "for i in mat:\n",
        "    if i == \"wiki\":\n",
        "        current_array = mat[i][0][0]\n",
        "        \n",
        "for j in range(len(current_array)):\n",
        "    df[columns[j]] = pd.DataFrame(current_array[j][0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zV0jtPsEqxHV",
        "colab_type": "code",
        "outputId": "08117352-6c45-49f2-f519-2e87d6c4f750",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        }
      },
      "source": [
        "df.head()               # Lets see how exactly our dataset looks"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>dob</th>\n",
              "      <th>photo_taken</th>\n",
              "      <th>full_path</th>\n",
              "      <th>gender</th>\n",
              "      <th>name</th>\n",
              "      <th>face_location</th>\n",
              "      <th>face_score</th>\n",
              "      <th>second_face_score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>723671</td>\n",
              "      <td>2009</td>\n",
              "      <td>[17/10000217_1981-05-05_2009.jpg]</td>\n",
              "      <td>1.0</td>\n",
              "      <td>[Sami Jauhojärvi]</td>\n",
              "      <td>[[111.29109473290997, 111.29109473290997, 252....</td>\n",
              "      <td>4.300962</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>703186</td>\n",
              "      <td>1964</td>\n",
              "      <td>[48/10000548_1925-04-04_1964.jpg]</td>\n",
              "      <td>1.0</td>\n",
              "      <td>[Dettmar Cramer]</td>\n",
              "      <td>[[252.48330229530742, 126.68165114765371, 354....</td>\n",
              "      <td>2.645639</td>\n",
              "      <td>1.949248</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>711677</td>\n",
              "      <td>2008</td>\n",
              "      <td>[12/100012_1948-07-03_2008.jpg]</td>\n",
              "      <td>1.0</td>\n",
              "      <td>[Marc Okrand]</td>\n",
              "      <td>[[113.52, 169.83999999999997, 366.08, 422.4]]</td>\n",
              "      <td>4.329329</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>705061</td>\n",
              "      <td>1961</td>\n",
              "      <td>[65/10001965_1930-05-23_1961.jpg]</td>\n",
              "      <td>1.0</td>\n",
              "      <td>[Aleksandar Matanović]</td>\n",
              "      <td>[[1, 1, 634, 440]]</td>\n",
              "      <td>-inf</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>720044</td>\n",
              "      <td>2012</td>\n",
              "      <td>[16/10002116_1971-05-31_2012.jpg]</td>\n",
              "      <td>0.0</td>\n",
              "      <td>[Diana Damrau]</td>\n",
              "      <td>[[171.61031405173117, 75.57451239763239, 266.7...</td>\n",
              "      <td>3.408442</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      dob  photo_taken  ... face_score  second_face_score\n",
              "0  723671         2009  ...   4.300962                NaN\n",
              "1  703186         1964  ...   2.645639           1.949248\n",
              "2  711677         2008  ...   4.329329                NaN\n",
              "3  705061         1961  ...       -inf                NaN\n",
              "4  720044         2012  ...   3.408442                NaN\n",
              "\n",
              "[5 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QNpvqnzoUL2n",
        "colab_type": "text"
      },
      "source": [
        "Converting DOB to Python format. We just need the birth year.\n",
        "\n",
        "PS: I copy-pasted this code :P"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yY5-DgNnqyou",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from datetime import datetime, timedelta\n",
        "def datenum_to_datetime(datenum):\n",
        "    days = datenum % 1\n",
        "    hours = days % 1 * 24\n",
        "    minutes = hours % 1 * 60\n",
        "    seconds = minutes % 1 * 60\n",
        "    exact_date = datetime.fromordinal(int(datenum)) \\\n",
        "    + timedelta(days=int(days)) + timedelta(hours=int(hours)) \\\n",
        "    + timedelta(minutes=int(minutes)) + timedelta(seconds=round(seconds)) \\\n",
        "    - timedelta(days=366)\n",
        "\n",
        "    return exact_date.year\n",
        " \n",
        "df['date_of_birth'] = df['dob'].apply(datenum_to_datetime)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MuyXHt_Xq0n4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['age'] = df['photo_taken'] - df['date_of_birth']          # Getting present age"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gru7UnYuq3-h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#remove pictures does not include face\n",
        "df = df[df['face_score'] != -np.inf]\n",
        " \n",
        "#some pictures include more than one face, remove them\n",
        "df = df[df['second_face_score'].isna()]\n",
        " \n",
        "#check threshold\n",
        "df = df[df['face_score'] >= 3]\n",
        " \n",
        "#some records do not have a gender information\n",
        "df = df[~df['gender'].isna()]\n",
        " \n",
        "df = df.drop(columns = ['name','face_score','second_face_score','date_of_birth','face_location'])\n",
        "\n",
        "#some guys seem to be greater than 100. some of these are paintings. remove these old guys\n",
        "df = df[df['age'] <= 100]\n",
        " \n",
        "#some guys seem to be unborn in the data set\n",
        "df = df[df['age'] > 0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m84G9Z35q5PN",
        "colab_type": "code",
        "outputId": "de92d1a9-e375-4b15-a1c4-4f4d67c32a1c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        }
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>dob</th>\n",
              "      <th>photo_taken</th>\n",
              "      <th>full_path</th>\n",
              "      <th>gender</th>\n",
              "      <th>age</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>723671</td>\n",
              "      <td>2009</td>\n",
              "      <td>[17/10000217_1981-05-05_2009.jpg]</td>\n",
              "      <td>1.0</td>\n",
              "      <td>28</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>711677</td>\n",
              "      <td>2008</td>\n",
              "      <td>[12/100012_1948-07-03_2008.jpg]</td>\n",
              "      <td>1.0</td>\n",
              "      <td>60</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>720044</td>\n",
              "      <td>2012</td>\n",
              "      <td>[16/10002116_1971-05-31_2012.jpg]</td>\n",
              "      <td>0.0</td>\n",
              "      <td>41</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>716189</td>\n",
              "      <td>2012</td>\n",
              "      <td>[02/10002702_1960-11-09_2012.jpg]</td>\n",
              "      <td>0.0</td>\n",
              "      <td>52</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>707745</td>\n",
              "      <td>1971</td>\n",
              "      <td>[41/10003541_1937-09-27_1971.jpg]</td>\n",
              "      <td>1.0</td>\n",
              "      <td>34</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      dob  photo_taken                          full_path  gender  age\n",
              "0  723671         2009  [17/10000217_1981-05-05_2009.jpg]     1.0   28\n",
              "2  711677         2008    [12/100012_1948-07-03_2008.jpg]     1.0   60\n",
              "4  720044         2012  [16/10002116_1971-05-31_2012.jpg]     0.0   41\n",
              "5  716189         2012  [02/10002702_1960-11-09_2012.jpg]     0.0   52\n",
              "6  707745         1971  [41/10003541_1937-09-27_1971.jpg]     1.0   34"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e4usZ8diq7OT",
        "colab_type": "code",
        "outputId": "0ad66faa-50ed-415c-eae7-1cbc10a7aff3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229
        }
      },
      "source": [
        "df.info()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 22138 entries, 0 to 62327\n",
            "Data columns (total 5 columns):\n",
            " #   Column       Non-Null Count  Dtype  \n",
            "---  ------       --------------  -----  \n",
            " 0   dob          22138 non-null  int32  \n",
            " 1   photo_taken  22138 non-null  uint16 \n",
            " 2   full_path    22138 non-null  object \n",
            " 3   gender       22138 non-null  float64\n",
            " 4   age          22138 non-null  int64  \n",
            "dtypes: float64(1), int32(1), int64(1), object(1), uint16(1)\n",
            "memory usage: 821.5+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ekOeTc2Lq9kC",
        "colab_type": "code",
        "outputId": "4cffcb9d-e8ef-4f3b-ac44-423b11417dc9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "source": [
        "histogram_age = df['age'].hist(bins=df['age'].nunique())            \n",
        "\n",
        "# Pretty obvious that a large chunk of population in training data is from 20 to 60\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ca2IU5Mfr3Sx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "2056c717-f901-4b85-db1c-12cc824529e8"
      },
      "source": [
        " df = df[::5]                    \n",
        " ''' Step size is 5. Actually there are 22138 entries in dataframe. But for computational reasons, i choose every 5th entry. Google colab has limited RAM \n",
        " (12 GB) and gets exhausted for any larger dataframe. Needless to say, running image dataset on local machine is not a good idea !''' "
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' Step size is 5. Actually there are 22138 entries in dataframe. But for computational reasons, i choose every 5th entry. Google colab has limited RAM \\n(12 GB) and gets exhausted for any larger dataframe. Needless to say, running image dataset on local machine is not a good idea !'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b_q9k75prBOq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Full path column states the exact location of the picture on the disk.We need its pixel values.\n",
        "\n",
        "target_size = (224, 224)\n",
        " \n",
        "def getImagePixels(image_path):\n",
        "    img = load_img(\"wiki_crop/%s\" % image_path[0], grayscale=False, target_size=target_size)\n",
        "    x = img_to_array(img).reshape(1, -1)[0]\n",
        "    return x\n",
        " \n",
        "df['pixels'] = df['full_path'].apply(getImagePixels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TORAAbT4rDLs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "classes = 101                                                # 0 to 100\n",
        "target = df['age'].values\n",
        "target_classes = utils.to_categorical(target, classes)\n",
        " \n",
        "features = []\n",
        " \n",
        "for i in range(0, df.shape[0]):\n",
        "  features.append(df['pixels'].values[i])\n",
        " \n",
        "features = np.array(features)\n",
        "features = features.reshape(features.shape[0], 224, 224, 3)\n",
        "\n",
        "# Splitting dataset as training and testing set. Essentially, here test set is basically\n",
        "# validation set. Actual test set is on what we apply model in real life (and is with \n",
        "# competition organizers in case of a competition)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "train_x, test_x, train_y, test_y = train_test_split(features, target_classes, test_size=0.30)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jky-ef-qQAvA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 247
        },
        "outputId": "e7330e47-3c1c-48f2-d09b-dd6039570154"
      },
      "source": [
        "df.info()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 4428 entries, 0 to 62323\n",
            "Data columns (total 6 columns):\n",
            " #   Column       Non-Null Count  Dtype  \n",
            "---  ------       --------------  -----  \n",
            " 0   dob          4428 non-null   int32  \n",
            " 1   photo_taken  4428 non-null   uint16 \n",
            " 2   full_path    4428 non-null   object \n",
            " 3   gender       4428 non-null   float64\n",
            " 4   age          4428 non-null   int64  \n",
            " 5   pixels       4428 non-null   object \n",
            "dtypes: float64(1), int32(1), int64(1), object(2), uint16(1)\n",
            "memory usage: 198.9+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vMvOFy5qVtOC",
        "colab_type": "text"
      },
      "source": [
        "# APPARENT  AGE   PREDICTION   MODEL\n",
        "Age prediction is a regression problem. But researchers define it as a classification problem. There are 101 classes in the output layer for ages 0 to 100. We apply transfer learning to exploit the closeness of this task with facial processing tasks. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kYqUhLsjsLco",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''VGG-Face model'''\n",
        "\n",
        "model = Sequential()\n",
        "# Remember it is VGG (transfer learning). Hence the input shape\n",
        "model.add(ZeroPadding2D((1,1),input_shape=(224,224, 3)))                    \n",
        "model.add(Convolution2D(64, (3, 3), activation='relu'))\n",
        "model.add(ZeroPadding2D((1,1)))\n",
        "model.add(Convolution2D(64, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
        " \n",
        "model.add(ZeroPadding2D((1,1)))\n",
        "model.add(Convolution2D(128, (3, 3), activation='relu'))\n",
        "model.add(ZeroPadding2D((1,1)))\n",
        "model.add(Convolution2D(128, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
        " \n",
        "model.add(ZeroPadding2D((1,1)))\n",
        "model.add(Convolution2D(256, (3, 3), activation='relu'))\n",
        "model.add(ZeroPadding2D((1,1)))\n",
        "model.add(Convolution2D(256, (3, 3), activation='relu'))\n",
        "model.add(ZeroPadding2D((1,1)))\n",
        "model.add(Convolution2D(256, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
        " \n",
        "model.add(ZeroPadding2D((1,1)))\n",
        "model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
        "model.add(ZeroPadding2D((1,1)))\n",
        "model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
        "model.add(ZeroPadding2D((1,1)))\n",
        "model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
        " \n",
        "model.add(ZeroPadding2D((1,1)))\n",
        "model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
        "model.add(ZeroPadding2D((1,1)))\n",
        "model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
        "model.add(ZeroPadding2D((1,1)))\n",
        "model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
        " \n",
        "model.add(Convolution2D(4096, (7, 7), activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Convolution2D(4096, (1, 1), activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Convolution2D(2622, (1, 1)))\n",
        "model.add(Flatten())\n",
        "model.add(Activation('softmax'))\n",
        "model.load_weights('/content/drive/My Drive/vgg_face_weights.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cdXU1bw9sbw2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for layer in model.layers[:-7]:\n",
        "  layer.trainable = False\n",
        " \n",
        " # We should lock the layer weights for early layers because they could already detect some patterns. \n",
        " # Since, we need 101 (0-100) units for age prediction,\n",
        " #  we add a custom convolution layer consisting of 101 units.\n",
        "\n",
        "base_model_output = Sequential()\n",
        "base_model_output = Convolution2D(101, (1, 1), name='predictions')(model.layers[-4].output)\n",
        "base_model_output = Flatten()(base_model_output)\n",
        "base_model_output = Activation('softmax')(base_model_output)\n",
        " \n",
        "age_model = Model(inputs=model.input, outputs=base_model_output)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jP3Sp3R7vEJI",
        "colab_type": "code",
        "outputId": "64925530-0787-4e4b-a7ee-386714b847e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "age_model.compile(loss='categorical_crossentropy', optimizer=optimizers.Adam(), metrics=['accuracy'])\n",
        " \n",
        "checkpointer = ModelCheckpoint(filepath='age_model.hdf5'\n",
        ", monitor = \"val_loss\", verbose=1, save_best_only=True, mode = 'auto')\n",
        " \n",
        "scores = []\n",
        "epochs = 250; batch_size = 256\n",
        " \n",
        "for i in range(epochs):\n",
        "  print(\"epoch \",i)\n",
        " \n",
        "ix_train = np.random.choice(train_x.shape[0], size=batch_size)\n",
        " \n",
        "score = age_model.fit(train_x[ix_train], train_y[ix_train]\n",
        ", epochs=1, validation_data=(test_x, test_y), callbacks=[checkpointer])\n",
        " \n",
        "scores.append(score)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch  0\n",
            "epoch  1\n",
            "epoch  2\n",
            "epoch  3\n",
            "epoch  4\n",
            "epoch  5\n",
            "epoch  6\n",
            "epoch  7\n",
            "epoch  8\n",
            "epoch  9\n",
            "epoch  10\n",
            "epoch  11\n",
            "epoch  12\n",
            "epoch  13\n",
            "epoch  14\n",
            "epoch  15\n",
            "epoch  16\n",
            "epoch  17\n",
            "epoch  18\n",
            "epoch  19\n",
            "epoch  20\n",
            "epoch  21\n",
            "epoch  22\n",
            "epoch  23\n",
            "epoch  24\n",
            "epoch  25\n",
            "epoch  26\n",
            "epoch  27\n",
            "epoch  28\n",
            "epoch  29\n",
            "epoch  30\n",
            "epoch  31\n",
            "epoch  32\n",
            "epoch  33\n",
            "epoch  34\n",
            "epoch  35\n",
            "epoch  36\n",
            "epoch  37\n",
            "epoch  38\n",
            "epoch  39\n",
            "epoch  40\n",
            "epoch  41\n",
            "epoch  42\n",
            "epoch  43\n",
            "epoch  44\n",
            "epoch  45\n",
            "epoch  46\n",
            "epoch  47\n",
            "epoch  48\n",
            "epoch  49\n",
            "epoch  50\n",
            "epoch  51\n",
            "epoch  52\n",
            "epoch  53\n",
            "epoch  54\n",
            "epoch  55\n",
            "epoch  56\n",
            "epoch  57\n",
            "epoch  58\n",
            "epoch  59\n",
            "epoch  60\n",
            "epoch  61\n",
            "epoch  62\n",
            "epoch  63\n",
            "epoch  64\n",
            "epoch  65\n",
            "epoch  66\n",
            "epoch  67\n",
            "epoch  68\n",
            "epoch  69\n",
            "epoch  70\n",
            "epoch  71\n",
            "epoch  72\n",
            "epoch  73\n",
            "epoch  74\n",
            "epoch  75\n",
            "epoch  76\n",
            "epoch  77\n",
            "epoch  78\n",
            "epoch  79\n",
            "epoch  80\n",
            "epoch  81\n",
            "epoch  82\n",
            "epoch  83\n",
            "epoch  84\n",
            "epoch  85\n",
            "epoch  86\n",
            "epoch  87\n",
            "epoch  88\n",
            "epoch  89\n",
            "epoch  90\n",
            "epoch  91\n",
            "epoch  92\n",
            "epoch  93\n",
            "epoch  94\n",
            "epoch  95\n",
            "epoch  96\n",
            "epoch  97\n",
            "epoch  98\n",
            "epoch  99\n",
            "epoch  100\n",
            "epoch  101\n",
            "epoch  102\n",
            "epoch  103\n",
            "epoch  104\n",
            "epoch  105\n",
            "epoch  106\n",
            "epoch  107\n",
            "epoch  108\n",
            "epoch  109\n",
            "epoch  110\n",
            "epoch  111\n",
            "epoch  112\n",
            "epoch  113\n",
            "epoch  114\n",
            "epoch  115\n",
            "epoch  116\n",
            "epoch  117\n",
            "epoch  118\n",
            "epoch  119\n",
            "epoch  120\n",
            "epoch  121\n",
            "epoch  122\n",
            "epoch  123\n",
            "epoch  124\n",
            "epoch  125\n",
            "epoch  126\n",
            "epoch  127\n",
            "epoch  128\n",
            "epoch  129\n",
            "epoch  130\n",
            "epoch  131\n",
            "epoch  132\n",
            "epoch  133\n",
            "epoch  134\n",
            "epoch  135\n",
            "epoch  136\n",
            "epoch  137\n",
            "epoch  138\n",
            "epoch  139\n",
            "epoch  140\n",
            "epoch  141\n",
            "epoch  142\n",
            "epoch  143\n",
            "epoch  144\n",
            "epoch  145\n",
            "epoch  146\n",
            "epoch  147\n",
            "epoch  148\n",
            "epoch  149\n",
            "epoch  150\n",
            "epoch  151\n",
            "epoch  152\n",
            "epoch  153\n",
            "epoch  154\n",
            "epoch  155\n",
            "epoch  156\n",
            "epoch  157\n",
            "epoch  158\n",
            "epoch  159\n",
            "epoch  160\n",
            "epoch  161\n",
            "epoch  162\n",
            "epoch  163\n",
            "epoch  164\n",
            "epoch  165\n",
            "epoch  166\n",
            "epoch  167\n",
            "epoch  168\n",
            "epoch  169\n",
            "epoch  170\n",
            "epoch  171\n",
            "epoch  172\n",
            "epoch  173\n",
            "epoch  174\n",
            "epoch  175\n",
            "epoch  176\n",
            "epoch  177\n",
            "epoch  178\n",
            "epoch  179\n",
            "epoch  180\n",
            "epoch  181\n",
            "epoch  182\n",
            "epoch  183\n",
            "epoch  184\n",
            "epoch  185\n",
            "epoch  186\n",
            "epoch  187\n",
            "epoch  188\n",
            "epoch  189\n",
            "epoch  190\n",
            "epoch  191\n",
            "epoch  192\n",
            "epoch  193\n",
            "epoch  194\n",
            "epoch  195\n",
            "epoch  196\n",
            "epoch  197\n",
            "epoch  198\n",
            "epoch  199\n",
            "epoch  200\n",
            "epoch  201\n",
            "epoch  202\n",
            "epoch  203\n",
            "epoch  204\n",
            "epoch  205\n",
            "epoch  206\n",
            "epoch  207\n",
            "epoch  208\n",
            "epoch  209\n",
            "epoch  210\n",
            "epoch  211\n",
            "epoch  212\n",
            "epoch  213\n",
            "epoch  214\n",
            "epoch  215\n",
            "epoch  216\n",
            "epoch  217\n",
            "epoch  218\n",
            "epoch  219\n",
            "epoch  220\n",
            "epoch  221\n",
            "epoch  222\n",
            "epoch  223\n",
            "epoch  224\n",
            "epoch  225\n",
            "epoch  226\n",
            "epoch  227\n",
            "epoch  228\n",
            "epoch  229\n",
            "epoch  230\n",
            "epoch  231\n",
            "epoch  232\n",
            "epoch  233\n",
            "epoch  234\n",
            "epoch  235\n",
            "epoch  236\n",
            "epoch  237\n",
            "epoch  238\n",
            "epoch  239\n",
            "epoch  240\n",
            "epoch  241\n",
            "epoch  242\n",
            "epoch  243\n",
            "epoch  244\n",
            "epoch  245\n",
            "epoch  246\n",
            "epoch  247\n",
            "epoch  248\n",
            "epoch  249\n",
            "Train on 256 samples, validate on 1329 samples\n",
            "Epoch 1/1\n",
            "256/256 [==============================] - 14s 55ms/step - loss: 12.8983 - accuracy: 0.0430 - val_loss: 7.1860 - val_accuracy: 0.0316\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 7.18597, saving model to age_model.hdf5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v8-zaxTpvLA9",
        "colab_type": "code",
        "outputId": "5f59f938-3cb9-477c-dd2e-59e36647ba28",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        " # Final testing of model (to see  how model performs on unseen data - albeit this data is \n",
        " # from the same distribution as training set. So, if model performs poorly on it, we \n",
        " # can conclude that model is overfitting)\n",
        "age_model.evaluate(test_x, test_y, verbose=1)  \n",
        "\n",
        "# Output of form - (loss, accuracy)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1329/1329 [==============================] - 5s 4ms/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[7.18597195146674, 0.031602710485458374]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HYITRnywSq41",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions = age_model.predict(test_x)          # Getting the predictions\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_DjiTlau3Tr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing import image\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        " \n",
        "def loadImage(filepath):\n",
        "  test_img = image.load_img(filepath, target_size=(224, 224))\n",
        "  test_img = image.img_to_array(test_img)\n",
        "  test_img = np.expand_dims(test_img, axis = 0)\n",
        "  test_img /= 255\n",
        "  return test_img\n",
        "\n",
        " # This picture should be in the same directory from which the nb is running\n",
        "picture = \"marlon-brando.jpg\"                                     \n",
        "prediction = age_model.predict(loadImage(picture))\n",
        "# Prediction variable stores distribution for each age class"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lHlIvoMEWYoB",
        "colab_type": "text"
      },
      "source": [
        "## Testing Model on custom images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5XJkXiP0WjRi",
        "colab_type": "text"
      },
      "source": [
        "# GENDER  PREDICTION  MODEL\n",
        "\n",
        "Gender Prediction is much easier than age prediction, primarily because it has less classes and gender related features are probably easier to learn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4EjrgXf4S95W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "target = df['gender'].values\n",
        "target_classes = utils.to_categorical(target, 2)                      #  Apply OHE to gender class\n",
        "\n",
        "for layer in model.layers[:-7]:\n",
        "  layer.trainable = False\n",
        " \n",
        "base_model_output = Sequential()\n",
        "base_model_output = Convolution2D(2, (1, 1), name='predictions')(model.layers[-4].output)\n",
        "base_model_output = Flatten()(base_model_output)\n",
        "base_model_output = Activation('softmax')(base_model_output)\n",
        " \n",
        "gender_model = Model(inputs=model.input, outputs=base_model_output)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UaTt1_6QTJjo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "scores = []\n",
        "epochs = 250; batch_size = 256\n",
        "gender_model.compile(loss='categorical_crossentropy', optimizer=optimizers.Adam(), metrics=['accuracy'])\n",
        "\n",
        "for i in range(epochs):\n",
        "  print(\"epoch \",i)\n",
        " \n",
        "ix_train = np.random.choice(train_x.shape[0], size=batch_size)\n",
        " \n",
        "score = gender_model.fit(train_x[ix_train], train_y[ix_train]\n",
        ", epochs=1, validation_data=(test_x, test_y), callbacks=[checkpointer])\n",
        " \n",
        "scores.append(score)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eqyR_7tDTT9B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gender_model.evaluate(test_x, test_y, verbose=1)                  # Evaluating on test set"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZATY4YGkTYd1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " # This picture should be in the same directory from which the nb is running\n",
        "picture = \"katy-perry.jpg\"                            \n",
        "prediction = gender_model.predict(loadImage(picture))\n",
        " \n",
        "img = image.load_img(picture)                          # target_size=(224, 224))\n",
        "plt.imshow(img)\n",
        "plt.show()\n",
        "gender = \"Male\" if np.argmax(prediction) == 1 else \"Female\"\n",
        "print(\"gender: \", gender)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}